[project]
name = "valhalla-databricks"
version = "1.0.0"
description = "Valhalla routing engine for Databricks with multi-cloud support"
readme = "README.md"
requires-python = ">=3.10,<3.13"
dependencies = [
    # databricks-connect version should match your DBR version (e.g., 16.4 for DBR 16.4 LTS)
    # It includes pyspark, so don't install pyspark separately
    # Python 3.10/3.11: use DBR 15.4-era connect client
    "databricks-connect>=15.4.0,<16.0.0; python_version < '3.12'",
    # Python 3.12: use DBR 16.4-era connect client
    "databricks-connect>=16.4.0,<17.0.0; python_version >= '3.12'",
    "databricks-sdk>=0.30.0",
    "pandas>=2.0.0",
    "python-dotenv>=1.0.0",
]

[build-system]
requires = ["hatchling"]
build-backend = "hatchling.build"

[tool.hatch.build.targets.wheel]
packages = ["src"]

[dependency-groups]
dev = [
    "pytest>=8.0.0",
    "pytest-cov>=4.0.0",
    "ruff>=0.4.0",
]

[tool.ruff]
line-length = 120
target-version = "py310"

[tool.ruff.lint]
select = ["E", "F", "I", "W"]
ignore = ["E501"]

[tool.pytest.ini_options]
testpaths = ["tests"]
pythonpath = ["src"]
addopts = "-v --tb=short"
filterwarnings = [
    # Suppress PySpark internal deprecation warnings (will be fixed in future PySpark versions)
    "ignore:distutils Version classes are deprecated:DeprecationWarning",
]

# Databricks Connect configuration
# This can be overridden with environment variables:
#   DATABRICKS_CONFIG_PROFILE - profile name from ~/.databrickscfg
#   DATABRICKS_CLUSTER_ID - cluster to connect to
#   DATABRICKS_SERVERLESS_COMPUTE_ID - use "auto" for serverless
#
# Configure your profile with: databricks auth login --profile <profile-name>

[tool.databricks-connect]
profile = "gcp"  # Or "azure", "aws" - matches your databricks.yml target
serverless_compute_id = "auto"

